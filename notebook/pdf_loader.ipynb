{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d135e55b",
   "metadata": {},
   "source": [
    "### RAG Pipelines Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b8952b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23210ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Loaded 129 documents from ../data\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs(pdf_directory : str):\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        pdf_directory,\n",
    "        glob = \"**/*.pdf\",\n",
    "        loader_cls = PyMuPDFLoader,\n",
    "        show_progress = False\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    for doc in documents:\n",
    "        doc.metadata['source_file'] = Path(doc.metadata['source']).name\n",
    "        doc.metadata['file_type'] = 'pdf'\n",
    "    print(f\"\\n✅ Loaded {len(documents)} documents from {pdf_directory}\")\n",
    "    return documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41c9e4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 5, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}, page_content='6 \\n \\n \\ncomputer for processing, which increases the response time. \\n➢ Lower Communication Cost  \\n• In distributed database systems, if data is located locally where it is mostly used, then \\nthe communication costs for data manipulation can be minimized.  \\n• This is not feasible in centralized systems. \\n \\nWhy Distributed Databases \\n➢ Organizational and economic reasons \\n➢ Interconnection of existing databases \\n➢ Incremental growth \\n➢ Reduced communication overhead \\n➢ Performance considerations \\n➢ Reliability and availability \\n \\nDifficult of Distributed Databases \\n➢ Need for complex and expensive software  \\nDDBMS demands complex and often expensive software to provide data transparency and co-\\nordination across the several sites. \\n \\n➢ Processing overhead \\nEven simple operations may require a large number of communications and additional \\ncalculations to provide uniformity in data across the sites. \\n \\n➢ Data integrity  \\nThe need for updating data in multiple sites pose problems of data integrity. \\n \\n➢ Overheads for improper data distribution \\nResponsiveness of queries is largely dependent upon proper data distribution. Improper data \\ndistribution often leads to very slow response to user requests. \\n \\nData Allocation \\nData Allocation is an intelligent distribution of your data pieces, (called data fragments) to \\nimprove database performance and Data Availability for end-users. It aims to reduce overall \\ncosts of transaction processing while also providing accurate data rapidly in your DDBMS \\nsystems. Data Allocation is one of the key steps in building your Distributed Database Systems. \\nThere are two common strategies used in optimal Data Allocation: Data Fragmentation and \\nData Replication. \\n \\nFragmentation and Replication In Distributed Database \\n \\nData Fragmentation \\n➢ Fragmentation is a process of disintegrating relations or tables into several partitions in \\nmultiple sites.  \\n➢ It divides a database into various subtables and sub relations so that data can be \\ndistributed and stored efficiently.  \\n➢ Database Fragmentation can be of two types: horizontal or vertical.  \\n➢ In a horizontal fragmentation, each tuple of a relation r is assigned to one or more \\nfragments.  \\n➢ In vertical fragmentation, the schema for a relation r is split into numerous smaller')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31918a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### process all pdfs inside directory \n",
    "\n",
    "# def process_all_pdfs(pdf_directory):\n",
    "#     ## process all pdf in the directory \n",
    "#     all_documents = []\n",
    "#     pdf_dir = Path(pdf_directory)\n",
    "\n",
    "#     ##find all pdf files recursively\n",
    "#     pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "#     print(f\"found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "#     for pdf_file in pdf_files:\n",
    "#         print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "#         try:\n",
    "#             loader = PyPDFLoader(str(pdf_file))\n",
    "#             documents = loader.load()\n",
    "\n",
    "#             ## add source info to metadata\n",
    "#             for doc in documents:\n",
    "#                 doc.metadata['source_file'] = pdf_file.name\n",
    "#                 doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "#             all_documents.extend(documents)\n",
    "#             print(f\" Loaded {len(documents)} pages\")\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\" Error: {e}\")\n",
    "            \n",
    "#     print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "#     return all_documents\n",
    "\n",
    "# # process all documents in data directory\n",
    "# all_pdf_documents = process_all_pdfs(\"../data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27ea8d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 0, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}\n",
      "{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 1, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}\n",
      "{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 2, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}\n",
      "{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 3, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}\n",
      "{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 4, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "for doc in all_pdf_documents[:5]:  # first 5 docs\n",
    "    print(doc.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17d9365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### text splitting into chunks\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    #splitting documents into smaller chunks for better RAG performance\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    #show example of chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "649fd977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 129 documents into 319 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: 1 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "SCHOOL OF COMPUTING \n",
      " \n",
      "DEPARTMENT OF COMPUTER SCIENCE AND \n",
      "ENGINEERING \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     \n",
      " \n",
      "    UNIT – I  - DISTRIBUTED DATABASE AND INFORMATION SY...\n",
      "Metadata: {'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 0, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb67c04",
   "metadata": {},
   "source": [
    "### Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22072edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a06cfa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedded model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimensions: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x130d76900>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    # handles document embedding generation using SentenceTransformer\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        # initialise embedding manager\n",
    "        #args: model_name = HuggingFace model for sentence embedding\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        # load SentenceTransformer model\n",
    "        try:\n",
    "            print(f\"Loading embedded model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimensions: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the model {self.model_name} : {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embedding(self, texts : List[str]) -> np.ndarray:\n",
    "        # generate embedding for list of texts\n",
    "\n",
    "        #args:\n",
    "            # texts: list of text strings to embeddings\n",
    "\n",
    "        # returns:\n",
    "            # numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not Loaded\")\n",
    "\n",
    "        print(f\"generate embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"generated embeddings with shape: {embeddings.shape}\")\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "## initialise the embeddding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d034f435",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93b295f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection : pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x130d76e40>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    # manage document embeddings in chromaDB vector store\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        # initialize the vector store\n",
    "\n",
    "        # Args:\n",
    "            # collection_name : name of chromaDB collection\n",
    "            # persist_directory : directory to persist the vector store\n",
    "        \n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        # initialize chromaDB client and collection\n",
    "        try:\n",
    "            #create persistent chromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path = self.persist_directory)\n",
    "\n",
    "            #get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata={\"description\" : \"PDF documents for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection : {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents : List[Any], embeddings: np.ndarray):\n",
    "        # add documents and their embeddings to the vector store\n",
    "\n",
    "        # Args:\n",
    "            #documents: list of LangChain documents\n",
    "            #embeddings: corresponding embeddings for the documents\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # prepare data for chromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # generate unique id\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids = ids,\n",
    "                embeddings = embeddings_list,\n",
    "                metadatas = metadatas,\n",
    "                documents = documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to Vector Store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding  documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb6d0095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 0, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}, page_content='1 \\n \\n \\n \\n \\n \\n \\nSCHOOL OF COMPUTING \\n \\nDEPARTMENT OF COMPUTER SCIENCE AND \\nENGINEERING \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n     \\n \\n    UNIT – I  - DISTRIBUTED DATABASE AND INFORMATION SYSTEMS- SCSA3008'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 1, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}, page_content='2 \\n \\n \\nSCSA3008_DISTRIBUTED DATABASE AND INFORMATION \\nSYSTEMS \\nCOURSE OBJECTIVES  \\n \\n➢ To understand the role of databases and database management systems in \\nmanaging organizational data and information. \\n➢ To understand the techniques used for data fragmentation, replication and \\nallocation during the distributed database design process. \\n➢ To discuss the issues involved in resource management and process. \\n➢ To Perceive the building blocks and design of information systems. \\n➢ To acquire knowledge of information systems on Business operations. \\n \\nCOURSE OUTCOMES  \\nOn completion of the course, student will be able to \\nCO1 - Identify the introductory distributed database concepts and its structures. \\nCO2 - Produce the transaction management and query processing techniques in  \\n           DDBMS. \\nCO3 - Develop in-depth understanding of relational databases and skills to optimize  \\n          database performance in practice. \\nCO4 - Critiques on each type of databases.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 1, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}, page_content='DDBMS. \\nCO3 - Develop in-depth understanding of relational databases and skills to optimize  \\n          database performance in practice. \\nCO4 - Critiques on each type of databases. \\nCO5 - Analyse, Design and present the information systems. \\nC06 - Designing of decision support system and tools for Business operations. \\n \\nUNIT 1      9 Hrs. \\nINTRODUCTORY CONCEPTS AND DESIGN OF (DDBMS) \\nData Fragmentation - Replication and allocation techniques for DDBMS - Methods \\nfor designing and implementing DDBMS - designing a distributed relational \\ndatabase - Architectures for DDBMS - Cluster federated - parallel databases and \\nclient server architecture - Overview of query processing. \\nUNIT 2    9 Hrs. \\nDISTRIBUTED \\nSECURITY \\nAND \\nDISTRIBUTED \\nDATABASE \\nAPPLICATION TECHNOLOGIES \\nOverview of security techniques - Cryptographic algorithms - Digital signatures - \\nDistributed Concurrency Control - Serializability theory - Taxonomy of concurrency'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 1, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}, page_content='APPLICATION TECHNOLOGIES \\nOverview of security techniques - Cryptographic algorithms - Digital signatures - \\nDistributed Concurrency Control - Serializability theory - Taxonomy of concurrency \\ncontrol mechanisms - Distributed deadlocks – Distributed Database Recovery - \\nDistributed Data Security - Web data management - Database Interoperability. \\nUNIT 3      9 Hrs \\nADVANCED IN DISTRIBUTED SYSTEMS \\nAuthentication in distributed systems - Protocols based on symmetric cryptosystems \\n- Protocols based on asymmetric cryptosystems - Password-based authentication - \\nUnstructured overlays - Chord distributed hash table – Content addressable'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2022-12-17T08:32:38+00:00', 'source': '../data/pdf_files/unit2.pdf', 'file_path': '../data/pdf_files/unit2.pdf', 'total_pages': 23, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2022-12-17T08:32:38+00:00', 'trapped': '', 'modDate': \"D:20221217083238+00'00'\", 'creationDate': \"D:20221217083238+00'00'\", 'page': 2, 'source_file': 'unit2.pdf', 'file_type': 'pdf'}, page_content='3 \\n \\n \\nnetworks (CAN) - Tapestry - Some other challenges in P2P system design - \\nTradeoffs between table storage and route lengths - Graph structures of complex \\nnetworks - Internet graphs - Generalized random graph networks. \\nUNIT 4       9 Hrs. \\nFUNDAMENTALAS OF INFORMATION SYSTEMS \\nDefining information – Classification of information – Presentation of information \\nsystems – Basics of Information systems –  Functions of information systems – \\nComponents of Information systems- Limitations of Information systems – \\nInformation System Design. \\nUNIT 5       9 Hrs. \\n ENTERPRISE COLLOBRATION SYSTEMS \\nGroupware – Types of groupware – Enterprise Communication tools – Enterprise \\nConferencing tools – Collaborative work management tools – Information System \\nfor Business operations – transaction processing systems – functional Information \\nSystems – Decision Support systems – Executive Information systems – Online \\nAnalytical processing.')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ae21c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate embeddings for 319 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10/10 [00:02<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (319, 384)\n",
      "Adding 319 documents to vector store...\n",
      "Successfully added 319 documents to Vector Store\n",
      "Total documents in collection: 319\n"
     ]
    }
   ],
   "source": [
    "# convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "# generate the embeddings\n",
    "embeddings = embedding_manager.generate_embedding(texts)\n",
    "\n",
    "# store it in the vector database\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
